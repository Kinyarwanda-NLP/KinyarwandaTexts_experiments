{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through a necessary step of any data science project - data cleaning. Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\". Feeding dirty data into a model will give us results that are meaningless.\n",
    "\n",
    "Specifically, we'll be walking through:\n",
    "\n",
    "1. **Getting the data -** in this case, we'll be scraping data from a website\n",
    "2. **Cleaning the data -** we will walk through popular text pre-processing techniques\n",
    "3. **Organizing the data -** we will organize the cleaned data into a way that is easy to input into other algorithms\n",
    "\n",
    "The output of this notebook will be clean, organized data in two standard text formats:\n",
    "\n",
    "1. **Corpus** - a collection of text\n",
    "2. **Document-Term Matrix** - word counts in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our goal is to look at transcripts of various comedians and note their similarities and differences. Specifically, I'd like to know if Ali Wong's comedy style is different than other comedians, since she's the comedian that got me interested in stand up comedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there are wonderful people online that keep track of stand up routine transcripts. [Scraps From The Loft](http://scrapsfromtheloft.com) makes them available for non-profit and educational purposes.\n",
    "\n",
    "To decide which comedians to look into, I went on IMDB and looked specifically at comedy specials that were released in the past 5 years. To narrow it down further, I looked only at those with greater than a 7.5/10 rating and more than 2000 votes. If a comedian had multiple specials that fit those requirements, I would pick the most highly rated one. I ended up with a dozen comedy specials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraping, pickle imports\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "# Scrapes transcript data from scrapsfromtheloft.com\n",
    "# def url_to_transcript(url):\n",
    "def url_to_transcript(url):\n",
    "    '''\n",
    "    This function get igihe article content and save the content on the txt file\n",
    "    '''\n",
    "    '''Returns transcript data specifically from http://m.igihe.com/'''\n",
    "    page =  requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    try :\n",
    "        article =  soup.find_all('div', class_=\"inkuru-title-2\")[0]\n",
    "    except IndexError as e:\n",
    "        print('Another IndexError: ', e)\n",
    "        return \n",
    "    paragraphs = article.find_all('p')\n",
    "    filename = url.rsplit('/', 1)[-1] + '.txt'\n",
    "\n",
    "    with open(os.path.join(filename), 'w+') as f:\n",
    "        for para in paragraphs:\n",
    "            f.write(para.getText()+ '\\n \\n')\n",
    "\n",
    "# URLs of transcripts in scope\n",
    "urls = ['http://m.igihe.com/imyidagaduro/article/nsengiyumva-igisupusupu-yasohoye-indirimbo-nshya-umutesi',\n",
    "        'http://m.igihe.com/amakuru/mu-mahanga/article/kiliziya-yahinduye-amabwiriza-agenga-ikurikiranwa-ry-abihayimana-bakekwaho',\n",
    "        'http://m.igihe.com/amakuru/muri-afurika/article/sudani-y-epfo-kiir-na-machar-bemeranyije-gushinga-guverinoma-y-ubumwe',\n",
    "        'http://m.igihe.com/imikino/football/article/apr-fc-yahize-rayon-sports-mu-ikipe-y-umwaka-wa-2019-sunrise-fc-ihagararira',\n",
    "        'https://mobile.igihe.com/ubukungu/ishoramari/article/inyungu-u-rwanda-rwiteze-mu-imurikagurisha-ry-ibikorerwa-muri-oman-rugiye',\n",
    "        'http://m.igihe.com/https://mobile.igihe.com/ubukungu/ishoramari/article/inyungu-u-rwanda-rwiteze-mu-imurikagurisha-ry-ibikorerwa-muri-oman-rugiye',\n",
    "        'http://m.igihe.com/amakuru/u-rwanda/article/nyamasheke-ibikorwaremezo-bike-ku-kiyaga-cya-kivu-imbogamizi-ku-iterambere',\n",
    "        'http://m.igihe.com/imyidagaduro/article/imbamutima-z-abatsinze-muri-hanga-higa-basabye-alain-muku-ko-yabatuza-i-kigali',\n",
    "        'http://m.igihe.com/amakuru/mu-mahanga/article/pervez-musharraf-wahoze-ayobora-pakistan-yakatiwe-igihano-cy-urupfu',\n",
    "        'http://m.igihe.com/ubukungu/article/abanyarwanda-bishimiye-icyemezo-cya-guverinoma-cyo-kuvanaho-tva-ku-bikoresho-by']\n",
    "\n",
    "# Article names (comedians=articles)\n",
    "articles = ['article1', 'article2', 'article3', 'article4', 'article5', 'article6', 'article7', 'article8', 'article9', 'article10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another IndexError:  list index out of range\n"
     ]
    }
   ],
   "source": [
    "# # Actually request transcripts (takes a few minutes to run)\n",
    "transcripts = [url_to_transcript(u) for u in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/desire/Documents/programming/github/kinyarwandaTexts'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "#stopwods : useless words ( uhmm, amm)\n",
    "#from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#rootdir = 'C:/Users/Sababu/Desktop/Kinyarwanda/Digital_Umuganda_data/words\n",
    "rootdir = '/home/desire/Documents/programming/github/kinyarwandaTexts/DATA/texts'\n",
    "db =[]\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith(\".txt\"):\n",
    "            rd = open(filepath, encoding=\"ISO-8859-1\").read().lower()\n",
    "            wrd = word_tokenize(rd)\n",
    "            words = [word for word in wrd if word.isalpha()]   \n",
    "                    \n",
    "            neww = words\n",
    "            db.extend(neww)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62803\n"
     ]
    }
   ],
   "source": [
    "def remove(list): \n",
    "    pattern = '[0-9]'\n",
    "    list = [re.sub(pattern, '', i) for i in list] \n",
    "    return list  \n",
    "\n",
    "new_file=remove(db) \n",
    "count = list(set(db)) \n",
    "\n",
    "print(len(db)) #all words in all .txt files including common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366200\n",
      "12255\n"
     ]
    }
   ],
   "source": [
    "#remove special characters & numbers\n",
    "new_filee = ''.join(e for e in new_file if e.isalnum()) \n",
    "print(len(new_filee))\n",
    "print(len(count))# unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#putting them into one file\n",
    "\n",
    "with open(\"words.txt\", \"w+\") as filehandle:\n",
    "    json.dump(count, filehandle,indent=2)\n",
    "\n",
    "\n",
    "    print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"duplicates.txt\", \"w+\") as filehandle:\n",
    "    json.dump(new_file, filehandle,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['muvuduko', 'gihangayikishije', 'kubicuranga', 'ihembwa', 'bashishikariza', 'kanobana', 'kisoro', 'gufasha', 'urubyiniro', 'ntoki']\n"
     ]
    }
   ],
   "source": [
    "print(count[2:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
    "\n",
    "**Common data cleaning steps on all text:**\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (/n)\n",
    "* Tokenize text\n",
    "* Remove stop words\n",
    "\n",
    "**More data cleaning steps after tokenization:**\n",
    "* Stemming / lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos\n",
    "* And more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine it!\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "data = pd.read_csv('words.txt', skiprows=1,  sep=\",\", header=None,  names=[\"words\", \"col\"])\n",
    "#data.columns =  [\"a\", \"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"rishake\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"muvuduko\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"gihangayikishije\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"kubicuranga\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"ihembwa\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  words\n",
       "1             \"rishake\"\n",
       "2            \"muvuduko\"\n",
       "3    \"gihangayikishije\"\n",
       "4         \"kubicuranga\"\n",
       "5             \"ihembwa\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data['col']\n",
    "\n",
    "data[1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12246</th>\n",
       "      <th>12247</th>\n",
       "      <th>12248</th>\n",
       "      <th>12249</th>\n",
       "      <th>12250</th>\n",
       "      <th>12251</th>\n",
       "      <th>12252</th>\n",
       "      <th>12253</th>\n",
       "      <th>12254</th>\n",
       "      <th>12255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>\"umucyo\"</td>\n",
       "      <td>\"rishake\"</td>\n",
       "      <td>\"muvuduko\"</td>\n",
       "      <td>\"gihangayikishije\"</td>\n",
       "      <td>\"kubicuranga\"</td>\n",
       "      <td>\"ihembwa\"</td>\n",
       "      <td>\"bashishikariza\"</td>\n",
       "      <td>\"kanobana\"</td>\n",
       "      <td>\"kisoro\"</td>\n",
       "      <td>\"gufasha\"</td>\n",
       "      <td>...</td>\n",
       "      <td>\"banki\"</td>\n",
       "      <td>\"byinjiza\"</td>\n",
       "      <td>\"bategura\"</td>\n",
       "      <td>\"awards\"</td>\n",
       "      <td>\"bahita\"</td>\n",
       "      <td>\"twahakekaga\"</td>\n",
       "      <td>\"ugabanuka\"</td>\n",
       "      <td>\"bugenzacyaha\"</td>\n",
       "      <td>\"bakanagirana\"</td>\n",
       "      <td>]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 12256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0            1             2                     3      \\\n",
       "words    \"umucyo\"    \"rishake\"    \"muvuduko\"    \"gihangayikishije\"   \n",
       "\n",
       "                 4            5                   6             7      \\\n",
       "words    \"kubicuranga\"    \"ihembwa\"    \"bashishikariza\"    \"kanobana\"   \n",
       "\n",
       "            8            9      ...      12246         12247         12248  \\\n",
       "words    \"kisoro\"    \"gufasha\"  ...    \"banki\"    \"byinjiza\"    \"bategura\"   \n",
       "\n",
       "            12249       12250            12251          12252  \\\n",
       "words    \"awards\"    \"bahita\"    \"twahakekaga\"    \"ugabanuka\"   \n",
       "\n",
       "                  12253             12254 12255  \n",
       "words    \"bugenzacyaha\"    \"bakanagirana\"     ]  \n",
       "\n",
       "[1 rows x 12256 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data).transpose()\n",
    "data_df = pd.DataFrame.from_dict(data).transpose()\n",
    "#data_df.columns = ['words']\n",
    "data_df = data_df.sort_index()\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"umucyo\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"rishake\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"muvuduko\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"gihangayikishije\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"kubicuranga\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12251</th>\n",
       "      <td>\"twahakekaga\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12252</th>\n",
       "      <td>\"ugabanuka\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12253</th>\n",
       "      <td>\"bugenzacyaha\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12254</th>\n",
       "      <td>\"bakanagirana\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255</th>\n",
       "      <td>]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12256 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      words\n",
       "0                  \"umucyo\"\n",
       "1                 \"rishake\"\n",
       "2                \"muvuduko\"\n",
       "3        \"gihangayikishije\"\n",
       "4             \"kubicuranga\"\n",
       "...                     ...\n",
       "12251         \"twahakekaga\"\n",
       "12252           \"ugabanuka\"\n",
       "12253        \"bugenzacyaha\"\n",
       "12254        \"bakanagirana\"\n",
       "12255                     ]\n",
       "\n",
       "[12256 rows x 1 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df.sort_index()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the updated text\n",
    "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
    "* Mark 'cheering' and 'cheer' as the same word (stemming / lemmatization)\n",
    "* Combine 'thank you' into one term (bi-grams)\n",
    "* And a lot more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned earlier that the output of this notebook will be clean, organized data in two standard text formats:\n",
    "1. **Corpus - **a collection of text\n",
    "2. **Document-Term Matrix - **word counts in matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our dataframe\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pickle it for later use\n",
    "df.to_pickle(\"corpus_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Add an additional regular expression to further clean the text\n",
    "2. Play around with CountVectorizer's parameters. ngram_range? min_df and max_df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
